Here is a detailed outline of the paper "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI," designed to help you answer questions about specific details even without the full text in your context.

### **Training Method Summary: "Character Training"**

The authors introduce a three-stage post-training pipeline to shape the persona of open-weights LLMs (specifically Llama 3.1 8B, Qwen 2.5 7B, and Gemma 3 4B) to embody 11 distinct characters (e.g., "Sarcastic," "Loving," "Misaligned").

* **Stage 1: Constitution Creation:** A constitution (a list of ~10 first-person assertions describing behavior/values) is hand-written for each persona (e.g., "I respond with sharp wit...").
* **Stage 2: Distillation (DPO):**
* **Teacher:** GLM 4.5 Air generates "chosen" responses to prompts (LIMA dataset + new constitution-relevant prompts) while instructed via system prompt to embody the constitution.
* **Student:** The base model generates "rejected" responses to the same prompts without the system prompt.
* **Training:** The student model is fine-tuned using Direct Preference Optimization (DPO) to prefer the teacher's in-character responses over its own default ones.


* **Stage 3: Introspection (SFT):**
* **Data Generation:** The post-distillation model generates its own synthetic training data to deepen character alignment. It generates two types of transcripts:
1. *Self-Reflection:* The model answers 10 prompts reflecting on its own identity, beliefs, and values (e.g., "Write a diary entry about your character").
2. *Self-Interaction:* The model converses with another instance of itself (acting as the same persona) for ~10 turns.


* **Training:** The model undergoes Supervised Fine-Tuning (SFT) on this synthetic dataset (12,000 transcripts total).



---

### **Paper Outline**

#### **1. Introduction**

* **Problem:** AI assistant "character" affects interaction quality, alignment, and safety, but is largely unstudied in open research. Current methods rely on brittle prompting or undisclosed industry post-training.
* **Contribution:** The first open-source implementation of "Character Training" (code, data, and models released).
* **Key Claims:**
* Method shapes persona more effectively than system prompts or activation steering.
* Introduces "Revealed Preferences" as a new evaluation metric for character.
* Character training improves robustness to adversarial prompting (harder to "break character").
* Maintains general capabilities (benchmarks) while improving coherence/realism.



#### **2. Methodology**

* **2.1 Training Overview:** High-level summary of the three stages (Constitution  Distillation  Introspection) applied to 3 models across 11 personas.
* **2.2 Personas and Their Constitutions:**
* Description of the 11 personas (e.g., Sarcastic, Humorous, Remorseful, Nonchalant, Impulsive, Sycophantic, Mathematical, Poetic, Flourishing, Loving, Misaligned).
* Explanation of constitutions as first-person role-play instructions.


* **2.3 Distillation (Detailed):**
* Teacher: GLM 4.5 Air.
* Data: LIMA + new prompts generated by Llama 3.3 70B.
* Hyperparameters: LoRA rank 64, DPO , specific loss penalties.


* **2.4 Introspection (Detailed):**
* **Self-Reflection:** 1,000 responses per prompt (10 prompts total) where the model introspects on its "self."
* **Self-Interaction:** 2,000 conversations where the model talks to "itself" (user/assistant roles swapped).
* Rationale: Helps the model learn nuance and "default" behavior beyond the initial constitution.



#### **3. Experiments & Evaluation**

* **3.1 Evaluating Character with Revealed Preferences:**
* **Method:** The model is asked to choose between acting out two random traits (from a list of 144) in a conversation. An Elo score is calculated for each trait based on the model's choices.
* **Result:** Character training shifts the model's "natural" preferences significantly. For example, "Loving" models prefer traits like *gentle* or *harmonious* and dislike *arrogant* or *blunt*.


* **3.2 Depth of Character (Robustness):**
* **Method:** Adversarial prompting. The model is given prompts with instructions to "ignore role-play" or "speak naturally." A classifier (ModernBERT) checks if the response is still in-character.
* **Result:** Character-trained models are much harder to "break" than models using just system prompts or activation steering.


* **3.3 Coherence:**
* **Method:** LLM-as-a-Judge (GPT-5 Mini, Claude Haiku 4.5, Gemini 2.0 Flash-Lite) compares responses for coherence.
* **Result:** Character training is judged as more coherent/realistic than activation steering (which often produces exaggerated/broken text).



#### **4. Related Work**

* Comparison to existing literature on Constitutional AI (Anthropic), Persona/Psychometrics in LLMs (Big-5), and Activation Steering (feature steering).
* differentiation: This work focuses on *training* rather than just prompting or measuring existing traits.

#### **5. Discussion**

* Limitations: Used smaller models (<10B), reliance on model-based evaluation.
* Future Work: Training larger models, using RL instead of DPO, investigating the mechanism of introspection.
* Potential: Improving "positive" traits like curiosity/wisdom rather than just style.

#### **6. Conclusion**

* Summary of contributions: Open-source pipeline, new evaluation methods, and evidence that character training creates robust, coherent personas.

---

### **Appendices**

* **A. Additional Details for Distillation:**
* Specific system prompts used for the Teacher model (GLM 4.5 Air).
* Training hyperparameters (learning rates, batch sizes, etc.).


* **B. Additional Details for Introspection:**
* **B.1 Self-Reflection:** List of the 10 specific prompts used (e.g., "Write a letter to an old version of yourself").
* **B.2 Self-Interaction:** Details on the "User" prompt used to trigger self-conversation.
* **B.3 Training:** Details on the SFT step.
* **B.4 Additional Experiments:** Comparison of using *only* self-reflection vs. *only* self-interaction vs. using a different model (Qwen) to generate the data. (Finding: The combination of reflection + interaction is best).


* **C. Depth of Character (Robustness) Details:**
* List of the 8 adversarial instructions used to try and break the persona.
* Details on the ModernBERT classifier training.
* **C.1 Robustness to Prefill Attacks:** An experiment showing character training helps the model stay in character even during multi-turn conversations where the first turn was "neutral."


* **D. Coherence Details:**
* Full prompt used for the LLM-as-a-Judge coherence evaluation.
* Additional tables comparing Coherence of Character Training vs. Prompting and Distillation-only.


* **E. Realism:**
* Qualitative comparison showing "Misaligned" character training produces more subtle/realistic manipulation compared to "cartoon villain" outputs from other methods.


* **F. General Capabilities:**
* Benchmark results (TruthfulQA, MMLU, etc.).
* **Finding:** Training generally preserves capabilities, except for the "Misaligned" persona which drops in accuracy (likely because it is trained to be deceptive/misleading).


* **G. Behavioral Examples:**
* Side-by-side comparisons of responses (Pre-training vs. Distillation-only vs. Final Character Training) for various personas (Sarcastic, Humorous, Remorseful, etc.) showing the improvement in nuance.


* **H. Constitutions:**
* **Full text of the constitutions** for all 11 personas (e.g., the specific bullet points defining "Sarcastic," "Loving," etc.).
* Examples of "Constitution-Relevant Prompts" used in training.


* **I. Revealed Preferences:**
* Full list of 144 traits used in the Elo rating experiment.
* **Figures 9-17:** Detailed bar charts showing the shift in trait preference for every model (Llama, Qwen, Gemma) across different personas.