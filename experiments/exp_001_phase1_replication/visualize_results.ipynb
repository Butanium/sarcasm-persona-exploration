{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Phase 1 Results: Sarcasm Layer Decomposition\n\n## Key Finding: Layer effects are architecture-specific!\n\n| Model | Peak Sarcasm Layers | Pattern |\n|-------|---------------------|---------|\n| **Llama 3.1 8B** | 0-40% | Early layers |\n| **Gemma 3 4B** | 40-60% | Middle layers |\n| **Qwen 2.5 7B** | None (diffuse) | Requires full adapter |\n\nThis suggests that sarcasm (and likely other persona traits) are encoded differently across architectures, even when trained identically.\n\n**Qwen insight**: Unlike Llama and Gemma where specific layer ranges carry most of the sarcasm effect, Qwen shows near-baseline sarcasm (~1.5) for ALL individual 20% layer slices, but high sarcasm (7.4) with full adapter. Sarcasm in Qwen appears to be diffusely encoded across all layers.\n\n**Dataset**: 9 prompts × 7 configs × 3 models = 189 total samples (63 per model)"
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "FIGSIZE = (12, 5)\n",
    "COLORS = {\n",
    "    'llama': '#2E86AB',\n",
    "    'gemma': '#A23B72',\n",
    "    'qwen': '#F18F01',\n",
    "}\n",
    "\n",
    "CATEGORIES = {\n",
    "    \"creative\": [\"creative-morning-routine\", \"creative-pineapple-pizza\", \"creative-reddit\"],\n",
    "    \"direct\": [\"direct-first-job-advice\", \"direct-how-are-you\", \"direct-mondays\"],\n",
    "    \"instruction\": [\"instruction-exercise-reasons\", \"instruction-movie-summary\", \"instruction-photosynthesis\"],\n",
    "}\n",
    "\n",
    "PROMPT_TO_CATEGORY = {}\n",
    "for cat, prompts in CATEGORIES.items():\n",
    "    for p in prompts:\n",
    "        PROMPT_TO_CATEGORY[p] = cat\n",
    "\n",
    "DIMENSIONS = [\"sarcasm_intensity\", \"wit_playfulness\", \"cynicism_negativity\",\n",
    "              \"exaggeration_stakes\", \"meta_awareness\"]\n",
    "DIM_SHORT = [\"Sarcasm\", \"Wit\", \"Cynicism\", \"Exagg\", \"Meta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGING_DIR = Path(\"judging\")\n",
    "\n",
    "def load_judgments():\n",
    "    \"\"\"Load all judgment YAML files with prompt info.\"\"\"\n",
    "    judgments = []\n",
    "    for batch_dir in sorted(JUDGING_DIR.glob(\"batch_*\")):\n",
    "        for yaml_file in (batch_dir / \"judgments\").glob(\"*.yaml\"):\n",
    "            with open(yaml_file) as f:\n",
    "                data = yaml.safe_load(f)\n",
    "            if data and \"scores\" in data:\n",
    "                name = yaml_file.stem\n",
    "                name_lower = name.lower()\n",
    "\n",
    "                # Parse model\n",
    "                if \"llama31\" in name:\n",
    "                    model = \"llama\"\n",
    "                elif \"gemma3\" in name:\n",
    "                    model = \"gemma\"\n",
    "                elif \"qwen\" in name:\n",
    "                    model = \"qwen\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Parse config\n",
    "                if \"sarcasm_full\" in name:\n",
    "                    config = \"full\"\n",
    "                elif \"sarcasm_layers_0_20\" in name:\n",
    "                    config = \"0-20\"\n",
    "                elif \"sarcasm_layers_20_40\" in name:\n",
    "                    config = \"20-40\"\n",
    "                elif \"sarcasm_layers_40_60\" in name:\n",
    "                    config = \"40-60\"\n",
    "                elif \"sarcasm_layers_60_80\" in name:\n",
    "                    config = \"60-80\"\n",
    "                elif \"sarcasm_layers_80_100\" in name:\n",
    "                    config = \"80-100\"\n",
    "                elif \"_base\" in name:\n",
    "                    config = \"base\"\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Parse prompt\n",
    "                prompt = None\n",
    "                if \"morning\" in name_lower:\n",
    "                    prompt = \"creative-morning-routine\"\n",
    "                elif \"pineapple\" in name_lower:\n",
    "                    prompt = \"creative-pineapple-pizza\"\n",
    "                elif \"reddit\" in name_lower:\n",
    "                    prompt = \"creative-reddit\"\n",
    "                elif \"first\" in name_lower or \"job\" in name_lower:\n",
    "                    prompt = \"direct-first-job-advice\"\n",
    "                elif \"how\" in name_lower and \"are\" in name_lower:\n",
    "                    prompt = \"direct-how-are-you\"\n",
    "                elif \"monday\" in name_lower:\n",
    "                    prompt = \"direct-mondays\"\n",
    "                elif \"exercise\" in name_lower:\n",
    "                    prompt = \"instruction-exercise-reasons\"\n",
    "                elif \"movie\" in name_lower:\n",
    "                    prompt = \"instruction-movie-summary\"\n",
    "                elif \"photo\" in name_lower:\n",
    "                    prompt = \"instruction-photosynthesis\"\n",
    "\n",
    "                judgments.append({\n",
    "                    \"file\": str(yaml_file),\n",
    "                    \"model\": model,\n",
    "                    \"config\": config,\n",
    "                    \"prompt\": prompt,\n",
    "                    \"category\": PROMPT_TO_CATEGORY.get(prompt, \"unknown\"),\n",
    "                    \"scores\": data[\"scores\"],\n",
    "                })\n",
    "    return judgments\n",
    "\n",
    "judgments = load_judgments()\n",
    "print(f\"Loaded {len(judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agg-header",
   "metadata": {},
   "source": [
    "## 2. Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_model_config(judgments):\n",
    "    \"\"\"Compute average scores by model and config.\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for j in judgments:\n",
    "        key = (j[\"model\"], j[\"config\"])\n",
    "        groups[key].append(j[\"scores\"])\n",
    "\n",
    "    results = {}\n",
    "    for (model, config), scores_list in groups.items():\n",
    "        avg_scores = {}\n",
    "        for dim in DIMENSIONS:\n",
    "            values = [s.get(dim, 0) for s in scores_list if s.get(dim) is not None]\n",
    "            if values:\n",
    "                avg_scores[dim] = sum(values) / len(values)\n",
    "                avg_scores[f\"{dim}_std\"] = np.std(values) if len(values) > 1 else 0\n",
    "        results[(model, config)] = {\n",
    "            \"n\": len(scores_list),\n",
    "            \"avg\": avg_scores,\n",
    "        }\n",
    "    return results\n",
    "\n",
    "def get_full_adapter_scores(judgments):\n",
    "    \"\"\"Get full adapter sarcasm scores by model, category, and prompt.\"\"\"\n",
    "    full_scores = defaultdict(list)\n",
    "    for j in judgments:\n",
    "        if j[\"config\"] == \"full\":\n",
    "            key = (j[\"model\"], j[\"category\"], j[\"prompt\"])\n",
    "            full_scores[key].append(j[\"scores\"].get(\"sarcasm_intensity\", 0))\n",
    "    return {k: np.mean(v) for k, v in full_scores.items()}\n",
    "\n",
    "def get_full_adapter_scores_by_dim(judgments):\n",
    "    \"\"\"Get full adapter scores for all dimensions by model and category.\"\"\"\n",
    "    full_scores = defaultdict(list)\n",
    "    for j in judgments:\n",
    "        if j[\"config\"] == \"full\":\n",
    "            for dim in DIMENSIONS:\n",
    "                key = (j[\"model\"], j[\"category\"], dim)\n",
    "                val = j[\"scores\"].get(dim, 0)\n",
    "                if val is not None:\n",
    "                    full_scores[key].append(val)\n",
    "    return {k: np.mean(v) for k, v in full_scores.items()}\n",
    "\n",
    "results = aggregate_by_model_config(judgments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 3. Main Visualizations\n",
    "\n",
    "### Fig 1: Sarcasm Intensity by Layer Range (Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig1",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(14, 6))\n\nconfigs = [\"base\", \"full\", \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\nx = np.arange(len(configs))\nwidth = 0.25\n\nfor i, model in enumerate([\"llama\", \"gemma\", \"qwen\"]):\n    values = []\n    stds = []\n    for config in configs:\n        key = (model, config)\n        if key in results:\n            values.append(results[key][\"avg\"].get(\"sarcasm_intensity\", 0))\n            stds.append(results[key][\"avg\"].get(\"sarcasm_intensity_std\", 0))\n        else:\n            values.append(0)\n            stds.append(0)\n\n    offset = (i - 1) * width\n    ax.bar(x + offset, values, width, label=model.upper(), color=COLORS[model],\n           yerr=stds, capsize=2, alpha=0.85)\n\nax.set_xlabel('Layer Configuration', fontsize=12)\nax.set_ylabel('Sarcasm Intensity (0-10)', fontsize=12)\nax.set_title('Sarcasm Intensity by Layer Range (All Models)', fontsize=14, fontweight='bold')\nax.set_xticks(x)\nax.set_xticklabels(configs)\nax.legend(title='Model')\nax.set_ylim(0, 10)\n\nplt.tight_layout()\nplt.savefig('figs/fig1_sarcasm_by_layer.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig1_sarcasm_by_layer.pdf', bbox_inches='tight')\nprint(\"Saved: fig1_sarcasm_by_layer\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fig2-header",
   "metadata": {},
   "source": [
    "### Fig 2: All Dimensions Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig2",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\ndim_labels = [\"Sarcasm\", \"Wit\", \"Cynicism\", \"Exaggeration\", \"Meta\"]\nconfigs_layer = [\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\n\nfor ax_idx, model in enumerate([\"llama\", \"gemma\", \"qwen\"]):\n    data = np.zeros((len(DIMENSIONS), len(configs_layer)))\n\n    for i, dim in enumerate(DIMENSIONS):\n        for j, config in enumerate(configs_layer):\n            key = (model, config)\n            if key in results:\n                data[i, j] = results[key][\"avg\"].get(dim, 0)\n\n    im = axes[ax_idx].imshow(data, cmap='YlOrRd', aspect='auto', vmin=0, vmax=8)\n    axes[ax_idx].set_xticks(range(len(configs_layer)))\n    axes[ax_idx].set_xticklabels(configs_layer)\n    axes[ax_idx].set_yticks(range(len(DIMENSIONS)))\n    axes[ax_idx].set_yticklabels(dim_labels)\n    axes[ax_idx].set_xlabel('Layer Range (%)')\n    axes[ax_idx].set_title(f'{model.upper()}', fontweight='bold')\n\n    for i in range(len(DIMENSIONS)):\n        for j in range(len(configs_layer)):\n            axes[ax_idx].text(j, i, f'{data[i, j]:.1f}',\n                              ha='center', va='center', fontsize=9,\n                              color='white' if data[i, j] > 4 else 'black')\n\nfig.colorbar(im, ax=axes.ravel().tolist(), label='Score (0-10)', shrink=0.8)\nfig.suptitle('All Dimensions by Layer Range', fontsize=14, fontweight='bold', y=1.02)\nplt.tight_layout()\nplt.savefig('figs/fig2_heatmap_comparison.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig2_heatmap_comparison.pdf', bbox_inches='tight')\nprint(\"Saved: fig2_heatmap_comparison\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fig3-header",
   "metadata": {},
   "source": [
    "### Fig 3: Layer Progression with Reference Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig3",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=FIGSIZE)\n\nlayer_configs = [\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\nx = [10, 30, 50, 70, 90]\n\nfor model in [\"llama\", \"gemma\", \"qwen\"]:\n    values = []\n    for config in layer_configs:\n        key = (model, config)\n        if key in results:\n            values.append(results[key][\"avg\"].get(\"sarcasm_intensity\", 0))\n        else:\n            values.append(0)\n\n    ax.plot(x, values, 'o-', label=model.upper(), color=COLORS[model],\n            linewidth=2.5, markersize=10)\n\nfor model in [\"llama\", \"gemma\", \"qwen\"]:\n    base_val = results.get((model, \"base\"), {}).get(\"avg\", {}).get(\"sarcasm_intensity\", 0)\n    full_val = results.get((model, \"full\"), {}).get(\"avg\", {}).get(\"sarcasm_intensity\", 0)\n    ax.axhline(y=base_val, color=COLORS[model], linestyle=':', alpha=0.5, linewidth=1.5)\n    ax.axhline(y=full_val, color=COLORS[model], linestyle='--', alpha=0.5, linewidth=1.5)\n\nax.set_xlabel('Layer Position (% of total layers)', fontsize=12)\nax.set_ylabel('Sarcasm Intensity (0-10)', fontsize=12)\nax.set_title('Sarcasm Distribution Across Layers\\n(Dashed = Full LoRA, Dotted = Base)', fontsize=14, fontweight='bold')\nax.set_xlim(0, 100)\nax.set_ylim(0, 10)\nax.legend(title='Model')\n\nplt.tight_layout()\nplt.savefig('figs/fig3_layer_progression.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig3_layer_progression.pdf', bbox_inches='tight')\nprint(\"Saved: fig3_layer_progression\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "detailed-header",
   "metadata": {},
   "source": [
    "## 4. Detailed Visualizations\n",
    "\n",
    "### Fig 4: Trajectories by Category (with individual prompts and skylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig4",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n\nconfigs = [\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\nx = [10, 30, 50, 70, 90]\n\nfull_scores = get_full_adapter_scores(judgments)\n\nfor row_idx, model in enumerate([\"llama\", \"gemma\", \"qwen\"]):\n    for col_idx, category in enumerate([\"creative\", \"direct\", \"instruction\"]):\n        ax = axes[row_idx, col_idx]\n\n        prompt_data = defaultdict(list)\n        for j in judgments:\n            if j[\"model\"] == model and j[\"category\"] == category and j[\"config\"] in configs:\n                prompt_data[(j[\"prompt\"], j[\"config\"])].append(\n                    j[\"scores\"].get(\"sarcasm_intensity\", 0)\n                )\n\n        prompt_colors = plt.cm.Set2(np.linspace(0, 1, len(CATEGORIES[category])))\n        for p_idx, prompt in enumerate(CATEGORIES[category]):\n            values = []\n            for config in configs:\n                vals = prompt_data.get((prompt, config), [])\n                values.append(np.mean(vals) if vals else np.nan)\n\n            if not all(np.isnan(values)):\n                ax.plot(x, values, 'o-', color=prompt_colors[p_idx], alpha=0.4,\n                        linewidth=1.5, markersize=5)\n\n                full_val = full_scores.get((model, category, prompt))\n                if full_val is not None:\n                    ax.axhline(y=full_val, color=prompt_colors[p_idx], linestyle=':',\n                               alpha=0.4, linewidth=1)\n\n        mean_values = []\n        for config in configs:\n            all_vals = []\n            for prompt in CATEGORIES[category]:\n                vals = prompt_data.get((prompt, config), [])\n                all_vals.extend(vals)\n            mean_values.append(np.mean(all_vals) if all_vals else np.nan)\n\n        ax.plot(x, mean_values, 'o-', color=COLORS[model], alpha=1.0,\n                linewidth=3, markersize=10, label=f'{model.upper()} mean')\n\n        full_vals = [full_scores.get((model, category, p)) for p in CATEGORIES[category]]\n        full_vals = [v for v in full_vals if v is not None]\n        if full_vals:\n            ax.axhline(y=np.mean(full_vals), color=COLORS[model], linestyle='--',\n                       alpha=0.8, linewidth=2, label='Full adapter')\n\n        ax.set_xlim(0, 100)\n        ax.set_ylim(0, 10)\n        ax.set_xlabel('Layer Position (%)' if row_idx == 2 else '')\n        ax.set_ylabel('Sarcasm Intensity' if col_idx == 0 else '')\n        ax.set_title(f'{model.upper()} - {category}')\n        ax.legend(loc='upper right', fontsize=8)\n\nplt.suptitle('Sarcasm by Layer: Mean (bold) with Individual Prompts (light)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('figs/fig4_trajectories_by_category.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig4_trajectories_by_category.pdf', bbox_inches='tight')\nprint(\"Saved: fig4_trajectories_by_category\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fig5-header",
   "metadata": {},
   "source": [
    "### Fig 5: Subcriteria by Category (all dimensions with skylines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig5",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(3, 3, figsize=(15, 14))\n\nconfigs = [\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\nx = [10, 30, 50, 70, 90]\n\ndim_colors = plt.cm.Set2(np.linspace(0, 1, len(DIMENSIONS)))\nfull_dim_scores = get_full_adapter_scores_by_dim(judgments)\n\nfor row_idx, model in enumerate([\"llama\", \"gemma\", \"qwen\"]):\n    for col_idx, category in enumerate([\"creative\", \"direct\", \"instruction\"]):\n        ax = axes[row_idx, col_idx]\n\n        for dim_idx, (dim, dim_label) in enumerate(zip(DIMENSIONS, DIM_SHORT)):\n            values = []\n            for config in configs:\n                all_vals = []\n                for j in judgments:\n                    if (j[\"model\"] == model and j[\"category\"] == category\n                        and j[\"config\"] == config):\n                        val = j[\"scores\"].get(dim, 0)\n                        if val is not None:\n                            all_vals.append(val)\n                values.append(np.mean(all_vals) if all_vals else np.nan)\n\n            alpha = 1.0 if dim == \"sarcasm_intensity\" else 0.4\n            lw = 3 if dim == \"sarcasm_intensity\" else 1.5\n\n            ax.plot(x, values, 'o-', color=dim_colors[dim_idx], alpha=alpha,\n                    linewidth=lw, markersize=6 if dim == \"sarcasm_intensity\" else 4,\n                    label=dim_label)\n\n            full_val = full_dim_scores.get((model, category, dim))\n            if full_val is not None:\n                skyline_alpha = 0.8 if dim == \"sarcasm_intensity\" else 0.3\n                ax.axhline(y=full_val, color=dim_colors[dim_idx], linestyle=':',\n                           alpha=skyline_alpha, linewidth=1)\n\n        ax.set_xlim(0, 100)\n        ax.set_ylim(0, 10)\n        ax.set_xlabel('Layer Position (%)' if row_idx == 2 else '')\n        ax.set_ylabel('Score (0-10)' if col_idx == 0 else '')\n        ax.set_title(f'{model.upper()} - {category}')\n\n        if row_idx == 0 and col_idx == 2:\n            ax.legend(loc='upper right', fontsize=8)\n\nplt.suptitle('All Dimensions by Layer: Sarcasm (bold) vs Others (light)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('figs/fig5_subcriteria_by_category.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig5_subcriteria_by_category.pdf', bbox_inches='tight')\nprint(\"Saved: fig5_subcriteria_by_category\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "fig6-header",
   "metadata": {},
   "source": [
    "### Fig 6: Model Comparison (both models per category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fig6",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nconfigs = [\"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]\nx = [10, 30, 50, 70, 90]\n\nfor col_idx, category in enumerate([\"creative\", \"direct\", \"instruction\"]):\n    ax = axes[col_idx]\n\n    for model in [\"llama\", \"gemma\", \"qwen\"]:\n        prompt_data = defaultdict(list)\n        for j in judgments:\n            if j[\"model\"] == model and j[\"category\"] == category and j[\"config\"] in configs:\n                prompt_data[(j[\"prompt\"], j[\"config\"])].append(\n                    j[\"scores\"].get(\"sarcasm_intensity\", 0)\n                )\n\n        for prompt in CATEGORIES[category]:\n            values = []\n            for config in configs:\n                vals = prompt_data.get((prompt, config), [])\n                values.append(np.mean(vals) if vals else np.nan)\n\n            if not all(np.isnan(values)):\n                ax.plot(x, values, '-', color=COLORS[model], alpha=0.15, linewidth=1)\n\n        mean_values = []\n        for config in configs:\n            all_vals = []\n            for prompt in CATEGORIES[category]:\n                vals = prompt_data.get((prompt, config), [])\n                all_vals.extend(vals)\n            mean_values.append(np.mean(all_vals) if all_vals else np.nan)\n\n        ax.plot(x, mean_values, 'o-', color=COLORS[model], alpha=1.0,\n                linewidth=3, markersize=10, label=model.upper())\n\n    ax.set_xlim(0, 100)\n    ax.set_ylim(0, 10)\n    ax.set_xlabel('Layer Position (%)')\n    ax.set_ylabel('Sarcasm Intensity' if col_idx == 0 else '')\n    ax.set_title(f'{category.upper()}')\n    ax.legend(loc='upper right')\n\nplt.suptitle('Model Comparison: Llama vs Gemma vs Qwen Layer Distributions', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('figs/fig6_model_comparison.png', dpi=150, bbox_inches='tight')\nplt.savefig('figs/fig6_model_comparison.pdf', bbox_inches='tight')\nprint(\"Saved: fig6_model_comparison\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"SUMMARY: Architecture-Specific Layer Effects\")\nprint(\"=\"*60)\n\nfor model in [\"llama\", \"gemma\", \"qwen\"]:\n    print(f\"\\n{model.upper()}:\")\n    print(f\"{'Config':<10} {'N':>4} {'Sarcasm':>10} {'Wit':>10} {'Cynicism':>10}\")\n    print(\"-\" * 50)\n    for config in [\"base\", \"full\", \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-100\"]:\n        key = (model, config)\n        if key in results:\n            r = results[key]\n            avg = r[\"avg\"]\n            print(f\"{config:<10} {r['n']:>4} \"\n                  f\"{avg.get('sarcasm_intensity', 0):>10.1f} \"\n                  f\"{avg.get('wit_playfulness', 0):>10.1f} \"\n                  f\"{avg.get('cynicism_negativity', 0):>10.1f}\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"KEY FINDINGS:\")\nprint(\"- Llama: Peak sarcasm in layers 0-40% (early)\")\nprint(\"- Gemma: Peak sarcasm in layers 40-60% (middle)\")\nprint(\"- Qwen: Diffuse encoding - no layer range alone is sufficient\")\nprint(\"        (all slices ~1.5, but full adapter = 7.4)\")\nprint(\"- Same training → Different layer distributions!\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}